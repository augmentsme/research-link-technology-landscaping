"""
Harmonization review module using InspectAI for keyword cluster validation.

This module provides functionality to review harmonization cluster proposals
generated by the embedding_harmonizer module and apply refinements based on
LLM review decisions.
"""

import json
from pathlib import Path
from typing import Dict, List, Union
from dataclasses import dataclass

# Inspect AI imports for the review task
from inspect_ai import Task, task
from inspect_ai.dataset import MemoryDataset, Sample
from inspect_ai.model import GenerateConfig, ResponseSchema
from inspect_ai.solver import system_message, generate
from inspect_ai.util import json_schema
from inspect_ai.hooks import Hooks, SampleEnd, hooks, TaskEnd
from inspect_ai.scorer import scorer, Score, Target, INCORRECT, CORRECT, accuracy, NOANSWER
from inspect_ai.solver import TaskState

from pydantic import BaseModel, Field

from config import (
    PROMPTS_DIR, 
    REVIEW_FILE, 
    CLUSTERS_PROPOSAL_PATH, 
    CLUSTERS_FINAL_PATH, 
    KEYWORDS_PATH, 
    EXTRACTED_KEYWORDS_PATH
)


@dataclass
class KeywordCluster:
    """Represents a cluster of harmonized keywords."""
    canonical_term: str
    variants: List[str]
    frequency: int


class ClusterReview(BaseModel):
    """Output model for reviewing a single cluster."""
    model_config = {"extra": "forbid"}
    
    cluster_id: int = Field(description="Index of the cluster being reviewed")
    canonical_term: str = Field(description="The canonical term of the cluster")
    variants: List[str] = Field(description="List of variant terms in the cluster")
    decision: str = Field(description="Decision: 'accept' or 'reject'")
    reasoning: str = Field(description="Explanation for the decision")


@scorer(metrics=[accuracy()])
def harmonization_reviewer():
    """
    Scorer that validates harmonization review decisions for single clusters.
    """
    
    async def score(state: TaskState, target: Target) -> Score:
        """Score based on review decision quality for a single cluster."""
        
        if not state.output or not state.output.completion:
            return Score(
                value=NOANSWER, 
                explanation="No harmonization review output to score"
            )
        
        try:
            result = json.loads(state.output.completion)
            
            # Expect single cluster review format
            decision = result.get("decision", "unknown")
            cluster_id = result.get("cluster_id", "unknown")
            reasoning = result.get("reasoning", "")
            
            # Validate decision
            if decision not in ["accept", "reject"]:
                return Score(
                    value=NOANSWER,
                    explanation=f"Invalid decision '{decision}' for cluster {cluster_id}. Expected 'accept' or 'reject'"
                )
            return Score(
                value=CORRECT if decision == "accept" else INCORRECT,
            )
                
        except (json.JSONDecodeError, KeyError, TypeError) as e:
            return Score(
                value=NOANSWER, 
                explanation=f"Error parsing harmonization review result: {str(e)}"
            )
    
    return score


@hooks(name="HarmonizationReviewHook", description="Hook to save harmonization review results and apply refinements")
class HarmonizationReviewHook(Hooks):
    """Hook to save harmonization review results and automatically apply refinements."""

    def __init__(self):
        super().__init__()
        self.reviews = []

    async def on_sample_end(self, data: SampleEnd) -> None:
        """Save individual cluster review result."""
        try:
            output_text = data.sample.output.completion
            result_json = json.loads(output_text)
            self.reviews.append(result_json)
        except (json.JSONDecodeError, KeyError, TypeError):
            pass  # Silently handle errors

    async def on_task_end(self, data: TaskEnd) -> None:
        """Aggregate all reviews and apply harmonization refinements."""
        try:
            # Save aggregated reviews in the expected format
            aggregated_result = {"reviews": self.reviews}
            with open(REVIEW_FILE, 'w', encoding='utf-8') as f:
                json.dump(aggregated_result, f, ensure_ascii=False)
            
            # Apply refinements
            apply_harmonization_review()
        except Exception:
            pass  # Silently handle errors


def load_harmonization_clusters() -> List[Dict]:
    """
    Load harmonization clusters from the proposal file.
    
    Returns:
        List of clusters with enhanced information for review.
    """
    clusters_file = CLUSTERS_PROPOSAL_PATH
    
    with open(clusters_file, 'r', encoding='utf-8') as f:
        cluster_data = json.load(f)
    
    # Convert to list of clusters with metadata for review
    clusters_for_review = []
    
    for cluster in cluster_data:
        # Extract information from variant keyword objects
        variants = cluster.get("variants", [])
        variant_terms = []
        descriptions_info = {}
        all_descriptions = []
        types = []
        
        for variant_obj in variants:
            if isinstance(variant_obj, dict):
                term = variant_obj.get("term", "")
                description = variant_obj.get("description", "No description available")
                variant_type = variant_obj.get("type", "general")
                
                variant_terms.append(term)
                descriptions_info[term] = description
                all_descriptions.append(description)
                types.append(variant_type)
            else:
                # Fallback for old string format
                variant_terms.append(str(variant_obj))
                descriptions_info[str(variant_obj)] = "No description available"
        
        review_cluster = {
            "cluster_id": cluster.get("cluster_id", cluster.get("id", 0)),
            "canonical_term": cluster["canonical_term"],
            "variants": variant_terms,
            "frequency": cluster["frequency"],
            "descriptions": descriptions_info,
            "all_descriptions": all_descriptions[:5],  # Limit to first 5 descriptions
            "types": types
        }
        clusters_for_review.append(review_cluster)
    
    return clusters_for_review


@task
def review() -> Task:
    """
    Review harmonization clusters to identify overly aggressive groupings.
    
    This task presents harmonization clusters to an LLM for review, identifying
    cases where specific terms are inappropriately grouped with general terms
    that could undermine emerging trends identification.
    
    NOTE: This task expects cluster proposals to already exist at CLUSTERS_PROPOSAL_PATH.
    Run embedding_harmonizer.py first to generate the proposals.
    """
    
    # Check if cluster proposals exist
    if not CLUSTERS_PROPOSAL_PATH.exists():
        raise FileNotFoundError(
            f"Cluster proposals file not found at {CLUSTERS_PROPOSAL_PATH}. "
            "Please run embedding_harmonizer.py first to generate cluster proposals."
        )
    
    # Load clusters for review
    clusters = load_harmonization_clusters()
    
    if not clusters:
        raise ValueError("No clusters found in proposals file.")
    
    # Create one sample per cluster for individual review
    samples = []
    for cluster in clusters:
        cluster_text = f"HARMONIZATION CLUSTER TO REVIEW:\n\n"
        cluster_text += f"Cluster {cluster['cluster_id']}:\n"
        cluster_text += f"  Canonical Term: {cluster['canonical_term']}\n"
        cluster_text += f"  Variants: {', '.join(cluster['variants'])}\n"
        cluster_text += f"  Frequency: {cluster['frequency']}\n"
        
        # Add type information if available
        types = cluster.get('types', [])
        if types:
            cluster_text += f"  Types: {', '.join(set(types))}\n"
        
        cluster_text += "\n"
        
        # Add descriptions for context
        cluster_text += "KEYWORD DESCRIPTIONS:\n"
        descriptions_info = cluster.get('descriptions', {})
        for variant, description in descriptions_info.items():
            cluster_text += f"  • {variant}: {description}\n"
        
        # Add sample of all descriptions if available
        all_descriptions = cluster.get('all_descriptions', [])
        if all_descriptions:
            cluster_text += f"\nSAMPLE DESCRIPTIONS FROM CLUSTER:\n"
            for i, desc in enumerate(all_descriptions[:3], 1):
                cluster_text += f"  {i}. {desc}\n"
        
        samples.append(Sample(
            id=f"cluster_{cluster['cluster_id']}",
            input=cluster_text,
            metadata={"cluster_id": cluster['cluster_id']}
        ))
    
    dataset = MemoryDataset(samples)
    
    return Task(
        dataset=dataset,
        solver=[
            system_message(str(PROMPTS_DIR / "harmonise.txt")),
            generate()
        ],
        scorer=[
            harmonization_reviewer()
        ],
        config=GenerateConfig(
            response_schema=ResponseSchema(
                name="single_cluster_review",
                json_schema=json_schema(ClusterReview),
                strict=True
            )
        ),
        hooks=["HarmonizationReviewHook"]
    )


def apply_harmonization_review() -> None:
    """
    Apply harmonization review decisions to create refined harmonization results.
    
    This function reads the review decisions and creates new harmonization files
    with problematic clusters removed (rejected clusters become individual terms).
    """
    
    # Load review decisions
    if not REVIEW_FILE.exists():
        raise FileNotFoundError(f"Review file not found at {REVIEW_FILE}. Run harmonise task first.")

    with open(REVIEW_FILE, 'r', encoding='utf-8') as f:
        review_data = json.load(f)
    
    reviews = review_data.get("reviews", [])
    
    # Load original cluster data
    clusters_file = CLUSTERS_PROPOSAL_PATH
    with open(clusters_file, 'r', encoding='utf-8') as f:
        original_clusters = json.load(f)
    
    # Create refined clusters based on review decisions
    refined_clusters = []
    
    for cluster in original_clusters:
        # Find corresponding review decision
        cluster_review = None
        
        # Extract variant terms from the cluster for comparison
        cluster_variant_terms = []
        variants = cluster.get("variants", [])
        for variant in variants:
            if isinstance(variant, dict):
                cluster_variant_terms.append(variant.get("term", ""))
            else:
                cluster_variant_terms.append(str(variant))
        
        for review in reviews:
            if (review.get("canonical_term") == cluster["canonical_term"] and 
                set(review.get("variants", [])) == set(cluster_variant_terms)):
                cluster_review = review
                break
        
        if cluster_review is None:
            # No review found, accept original cluster
            refined_clusters.append(cluster)
            continue
        
        decision = cluster_review.get("decision", "accept")
        
        if decision == "accept":
            # Accept cluster as is
            refined_clusters.append(cluster)
        
        elif decision == "reject":
            # Reject cluster - treat each variant as its own term
            # Don't include the cluster (variants will remain as individual terms)
            continue
        
        else:
            # Unknown decision, default to accept
            refined_clusters.append(cluster)
    
    # Save refined clusters
    refined_clusters_file = CLUSTERS_FINAL_PATH
    with open(refined_clusters_file, 'w', encoding='utf-8') as f:
        json.dump(refined_clusters, f, ensure_ascii=False)
    
    print(f"✅ Refined clusters saved to {refined_clusters_file}")
    
    # Create refined terms file for categorisation
    refined_terms_file = KEYWORDS_PATH
    # Load original keywords to rebuild mapping
    with open(EXTRACTED_KEYWORDS_PATH, 'r', encoding='utf-8') as f:
        initial_keywords = json.load(f)
    
    # Create mapping from refined clusters and use unified function
    mapping = create_mapping_from_clusters(refined_clusters)
    create_harmonized_keywords_file(initial_keywords, mapping, refined_terms_file)
    
    print(f"✅ Harmonized keywords saved to {refined_terms_file}")


def create_harmonized_keywords_file(initial_keywords: List[Dict], mapping: Dict[str, str], output_file: Union[str, Path]) -> None:
    """
    Create a harmonized/refined keywords file by replacing variants with canonical terms.
    Handles the new flattened keyword structure.
    
    Args:
        initial_keywords: Original flattened keywords data (list of keyword objects)
        mapping: Dictionary mapping variant terms to canonical terms
        output_file: Output file path
    """
    # Create case-insensitive mapping for lookup
    case_insensitive_mapping = {}
    for variant, canonical in mapping.items():
        case_insensitive_mapping[variant.lower()] = canonical
    
    # Process original keywords and replace with canonical terms
    harmonized_keywords = []
    seen_terms = set()
    
    for keyword in initial_keywords:
        if isinstance(keyword, dict) and 'term' in keyword:
            original_term = keyword['term']
            # Use case-insensitive lookup for canonical term
            canonical_term = case_insensitive_mapping.get(original_term.lower(), original_term)
            
            # Avoid duplicates - only keep first occurrence of each canonical term
            if canonical_term.lower() not in seen_terms:
                harmonized_keyword = {
                    'id': keyword.get('id', ''),
                    'term': canonical_term,
                    'type': keyword.get('type', 'general'),
                    'description': keyword.get('description', '')
                }
                harmonized_keywords.append(harmonized_keyword)
                seen_terms.add(canonical_term.lower())

    # Save harmonized keywords file
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(harmonized_keywords, f, ensure_ascii=False)


def create_mapping_from_clusters(refined_clusters: List) -> Dict[str, str]:
    """
    Create a mapping dictionary from refined clusters data.
    
    Args:
        refined_clusters: List of refined cluster data
        
    Returns:
        Dictionary mapping variant terms to canonical terms
    """
    mapping = {}
    for cluster in refined_clusters:
        canonical_term = cluster["canonical_term"]
        variants = cluster["variants"]
        
        for variant in variants:
            if isinstance(variant, dict):
                variant_term = variant.get("term", "")
                if variant_term:
                    mapping[variant_term] = canonical_term
            else:
                # Fallback for old string format
                mapping[str(variant)] = canonical_term
    return mapping


def main():
    """
    Main function to demonstrate usage.
    
    This function shows how to run the harmonization review process
    after cluster proposals have been generated.
    """
    print("Harmonization Review Module")
    print("==========================")
    print()
    print("This module provides InspectAI-based review of cluster proposals.")
    print("To use this module:")
    print()
    print("1. First run embedding_harmonizer.py to generate cluster proposals")
    print("2. Then run this harmonise task to review the proposals")
    print("3. Results will be saved to refined clusters and keywords files")
    print()
    print("Example usage:")
    print("  python embedding_harmonizer.py  # Generate proposals")
    print("  inspect eval harmonise.py@harmonise  # Review proposals")
    print()
    
    # Check if proposals exist
    if CLUSTERS_PROPOSAL_PATH.exists():
        print(f"✅ Cluster proposals found at {CLUSTERS_PROPOSAL_PATH}")
        clusters = load_harmonization_clusters()
        print(f"   {len(clusters)} clusters ready for review")
    else:
        print(f"❌ No cluster proposals found at {CLUSTERS_PROPOSAL_PATH}")
        print("   Run embedding_harmonizer.py first to generate proposals")


if __name__ == "__main__":
    main()
