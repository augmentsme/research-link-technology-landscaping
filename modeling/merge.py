from inspect_ai import Task, task
from itertools import batched
import config

from inspect_ai.hooks import Hooks, TaskEnd, hooks, SampleEnd
from inspect_ai.dataset import MemoryDataset, Sample
from inspect_ai.solver import system_message, generate, TaskState
from inspect_ai.model import GenerateConfig, ResponseSchema, get_model
from inspect_ai.util import json_schema
from inspect_ai.scorer import scorer, Target, Score, mean, Scorer
import json
import jsonlines
from pydantic import BaseModel, Field

from models import Category, CategoryList
import utils
from scorer import keywords_confusion_matrix
import re

import utils

@hooks(name="MergeCategoriesHook", description="Hook to combine FOR group merge results into a single categories.json")
class MergeCategoriesHook(Hooks):
    def __init__(self):
        # self.results = []
        # Create merge-specific paths for unknown and missing keywords
        # merge_unknown_path = config.Categories.category_dir / "merge_unknown_keywords.jsonl"
        # merge_missing_path = config.Categories.category_dir / "merge_missing_keywords.jsonl"

        self.unknown_write = jsonlines.open(config.Categories.merge_unknown_path, 'a')
        self.missing_write = jsonlines.open(config.Categories.merge_missing_path, 'a')
        self.category_write = jsonlines.open(config.Categories.categories_path, 'a')
        
        # Track completed batches
        self.completion_tracking_path = config.Categories.category_dir / "completed_batches.jsonl"
        self.completion_write = jsonlines.open(self.completion_tracking_path, 'a')
        
    async def on_sample_end(self, data: SampleEnd):
        _, result = utils.parse_results(data.sample.output.completion)
        
        # Add batch tracking information to each category
        batch_id = data.sample.metadata.get('batch_id', data.sample.id)
        for category in result['categories']:
            category['source_batch_id'] = batch_id
            category['source_field_of_research'] = data.sample.metadata.get('field_of_research')
            category['source_cluster_id'] = data.sample.metadata.get('cluster_id')
        
        # self.results.extend(result['categories'])
        self.category_write.write_all(result['categories'])
        
        # Track batch completion
        batch_completion_info = {
            'batch_id': batch_id,
            'field_of_research': data.sample.metadata.get('field_of_research'),
            'cluster_id': data.sample.metadata.get('cluster_id'),
            'categories_produced': len(result['categories']),
            'keywords_count': len(data.sample.metadata.get('keywords', []))
        }
        self.completion_write.write(batch_completion_info)
        
        # Save missing keywords from the keywords_confusion_matrix scorer
        if 'keywords_confusion_matrix' in data.sample.scores:
            missing_keywords = data.sample.scores['keywords_confusion_matrix'].metadata.get('fn', [])
            if missing_keywords:
                self.missing_write.write_all(missing_keywords)
        
        # Save unknown keywords (keywords in "Unknown" category)
        for category in result['categories']:
            if category['name'] == "Unknown":
                self.unknown_write.write_all(category['keywords'])

    async def on_task_end(self, data: TaskEnd) -> None:
        self.category_write.close()
        self.unknown_write.close()
        self.missing_write.close()
        self.completion_write.close()

def load_category_files_dataset() -> MemoryDataset:
    """
    Load clusters from the JSON file generated by the cluster command.
    
    This function expects clusters to be pre-generated using:
        python semantic_clustering.py cluster [--batch-size N]
    
    The clusters are loaded from the JSON file and converted into samples for merging.
    Skips batches that have already been processed.
    """
    # Load clusters from the JSON file generated by the cluster command
    clusters_output_path = config.Categories.category_dir / "clusters_by_for.json"
    
    if not clusters_output_path.exists():
        raise FileNotFoundError(
            f"Clusters file not found at {clusters_output_path}. "
            f"Please run 'python semantic_clustering.py cluster' first to generate clusters."
        )
    
    # Load clustering results
    with open(clusters_output_path, 'r') as f:
        clusters_data = json.load(f)
    
    # Check which batches have already been processed
    processed_batch_ids = set()
    
    # Check existing categories file for completed batches
    if config.Categories.categories_path.exists():
        try:
            existing_categories = utils.load_jsonl_file(config.Categories.categories_path, as_dataframe=False)
            for category in existing_categories:
                # Extract batch ID from category metadata if available
                if 'source_batch_id' in category:
                    processed_batch_ids.add(category['source_batch_id'])
        except Exception as e:
            print(f"Warning: Could not load existing categories file: {e}")
    
    # Also check for any existing sample completion markers in a separate tracking file
    completion_tracking_path = config.Categories.category_dir / "completed_batches.jsonl"
    if completion_tracking_path.exists():
        try:
            completed_batches = utils.load_jsonl_file(completion_tracking_path, as_dataframe=False)
            for batch_info in completed_batches:
                processed_batch_ids.add(batch_info['batch_id'])
        except Exception as e:
            print(f"Warning: Could not load completion tracking file: {e}")
    
    samples = []
    total_batches = 0
    skipped_batches = 0
    
    # Create samples from loaded clusters
    for field_of_research, field_data in clusters_data.items():
        clusters = field_data['clusters']
        
        for cluster_id, cluster_categories in clusters.items():
            total_batches += 1
            batch_id = f"merge_for_{field_of_research}_cluster_{cluster_id}"
            
            # Skip if this batch has already been processed
            if batch_id in processed_batch_ids:
                skipped_batches += 1
                continue
            
            cat_text = "\n".join([config.Categories.template(category) for category in cluster_categories])
            
            metadata = {
                "field_of_research": field_of_research,
                "keywords": [kw for category in cluster_categories for kw in category.get('keywords', [])]
            }
            
            # Add cluster info and batch tracking
            metadata["cluster_id"] = cluster_id
            metadata["total_proposals_in_cluster"] = len(cluster_categories)
            metadata["batch_id"] = batch_id
            
            sample = Sample(
                id=batch_id,
                input=cat_text,
                metadata=metadata
            )
            samples.append(sample)

    print(f"Total batches: {total_batches}")
    print(f"Skipped (already processed): {skipped_batches}")
    print(f"Created {len(samples)} new batches to process")
    return MemoryDataset(samples)


SYSTEM_PROMPT = f"""
You are an expert Taxonomy Specialist and Research Category Harmonization Expert, specializing in identifying and merging identical research categories to create unified, coherent taxonomies for strategic analysis.

Your task is to analyze the provided categorization data and identify categories that are identical or nearly identical, then combine them into a unified taxonomy. This requires systematic analysis of category names, descriptions, keywords, and FOR codes to determine optimal merging strategies.

**IMPORTANT: You MUST complete this task immediately and provide the full merged taxonomy. Do NOT ask for clarification, approval, or propose alternative approaches. Proceed directly with merging all provided categories.**

**Core Objective:**
Your primary goal is to consolidate duplicate or highly similar categories while preserving the integrity and completeness of the original keyword coverage. Your analysis should create a clean, non-redundant taxonomy that maintains all original information.

---

### **CRITICAL RULES FOR CATEGORY MERGING:**

**1. Category Identification & Merging Logic:**
*   **Similarity Assessment:** Identify categories as merge candidates based on substantial overlap in names, conceptual scope, and keyword sets. Categories with keyword overlaps or identical conceptual domains should be merged.
*   **Name Selection:** When merging categories, select the most descriptive and comprehensive name. If names are equally descriptive, choose the one that best captures the merged keyword set.
*   **Description Synthesis:** Create unified descriptions that incorporate the best elements from all merged categories, ensuring comprehensive coverage of the merged keyword scope.

**2. Keyword Handling & Coverage:**
*   **Complete Coverage:** EVERY keyword from ALL source categories MUST be included in exactly one merged category. No keywords should be lost during the merging process.
*   **Deduplication:** Remove duplicate keywords when merging categories, ensuring each keyword appears only once in the final taxonomy.
*   **Exact Term Preservation:** Maintain the exact keyword terms as they appeared in the source categories.

**3. Output JSON Structure & Quality:**
*   **Field of Research (FOR) Consistency:** Merged categories must maintain consistent FOR assignment. If merging categories with different FOR, select the one that best represents the majority of keywords or the most encompassing domain.
*   **Category Naming:** Each merged category requires a clear, descriptive `name` that accurately represents all included keywords.
*   **High-Quality Descriptions:** The `description` for each merged category must be comprehensive and insightful, explaining the category's scope and directly reflecting all keywords it contais.

**4. Quality Assurance & Validation:**
*   **Uniqueness Verification:** Ensure no two merged categories have substantial overlap in their keyword sets or conceptual scope.
*   **Completeness Check:** Verify that all source keywords are accounted for in the merged taxonomy.
*   **Coherence Assessment:** Each merged category should represent a coherent research domain that makes logical sense as a unified entity.
"""
@task
def merge() -> Task:
    """
    Merge categories using pre-generated clusters from semantic_clustering.py.
    
    Before running this task, you must first generate clusters using:
    python semantic_clustering.py cluster [--n-clusters N]
    
    Args:
        proposal_path: Path to category proposals (used for validation, clusters are loaded from JSON)
    """
    
    dataset = load_category_files_dataset()

    return Task(
        dataset=dataset,
        solver=[
            system_message(SYSTEM_PROMPT),
            generate()
        ],
        config=GenerateConfig(
            response_schema=ResponseSchema(
                name="MergedCategories",
                json_schema=json_schema(CategoryList),
                strict=True
            )
        ),
        scorer=keywords_confusion_matrix(),
        hooks=["MergeCategoriesHook"]
    )


