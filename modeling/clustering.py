"""
DBSCAN clustering module for keyword harmonization.

This module provides DBSCAN clustering functionality for keyword embeddings,
working with embeddings generated by ChromaDB with vLLM embedding functions.

Key features:
- DBSCAN clustering algorithm with cosine distance
- Keyword cluster management and caching
- CLI interface for clustering operations
- Direct ChromaDB integration for embedding retrieval
"""

import json
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Union, Optional, Tuple
from dataclasses import dataclass

from sklearn.cluster import DBSCAN
import numpy as np
import typer
import chromadb

from chromadb.utils.embedding_functions.openai_embedding_function import OpenAIEmbeddingFunction
from openai import OpenAI
from config import (
    KEYWORDS_EMBEDDING_DBPATH,
    EXTRACTED_KEYWORDS_PATH,
    CLUSTERS_PROPOSAL_PATH,
    SIMILARITY_THRESHOLD,
    MIN_CLUSTER_SIZE
)
@dataclass
class KeywordCluster:
    """Represents a cluster of harmonized keywords."""
    canonical_term: str
    variants: List[str]
    frequency: int


class ClusteringManager:
    """
    DBSCAN clustering manager for keyword harmonization.
    
    This class handles the clustering workflow:
    1. Load embeddings directly from ChromaDB
    2. Run DBSCAN clustering on embeddings
    3. Generate keyword clusters with canonical terms
    4. Cache and manage clustering results
    """

    def __init__(
        self, 
        similarity_threshold: float = 0.7, 
        min_cluster_size: int = 2,
        vllm_base_url: str = "http://localhost:8000/v1"
    ):
        self.similarity_threshold = similarity_threshold
        self.min_cluster_size = min_cluster_size
        self.vllm_base_url = vllm_base_url
        
        # ChromaDB components
        self.chroma_client = None
        self.chroma_collection = None
        self.collection_name = None  # Will be auto-detected from available collections
        
        # Cached clustering results
        self.cached_clusters = None

    def find_embedding_collection(self) -> str:
        """Find the most recent embedding collection based on model name."""
        try:
            collections = self.chroma_client.list_collections()
            
            # In ChromaDB v0.6+, list_collections returns collection names directly
            collection_names = [str(col) for col in collections]
            
            # Filter for embedding collections
            embedding_collections = [name for name in collection_names if name.startswith("embeddings_")]
            
            if not embedding_collections:
                # Fall back to old naming scheme
                legacy_collections = [name for name in collection_names if name == "keyword_embeddings"]
                if legacy_collections:
                    return legacy_collections[0]
                raise Exception("No embedding collections found. Run 'python embedding.py embed' first.")
            
            # If multiple collections exist, use the one with the most embeddings
            # This assumes the most recent/complete model embedding
            best_collection = None
            max_count = 0
            
            for col_name in embedding_collections:
                try:
                    # Get model name from vLLM server
                    temp_client = OpenAI(api_key="EMPTY", base_url=self.vllm_base_url)
                    models = temp_client.models.list()
                    if models.data:
                        model_name = models.data[0].id
                        temp_embedding_function = OpenAIEmbeddingFunction(
                            api_key="EMPTY",
                            model_name=model_name,
                            api_base=self.vllm_base_url
                        )
                        temp_collection = self.chroma_client.get_collection(
                            name=col_name,
                            embedding_function=temp_embedding_function
                        )
                        count = temp_collection.count()
                        if count > max_count:
                            max_count = count
                            best_collection = col_name
                except Exception:
                    continue
            
            if best_collection:
                print(f"üì¶ Auto-detected embedding collection: {best_collection} ({max_count} embeddings)")
                return best_collection
            else:
                # Just use the first one if we can't count them
                collection_name = embedding_collections[0]
                print(f"üì¶ Using embedding collection: {collection_name}")
                return collection_name
                
        except Exception as e:
            raise Exception(f"Failed to find embedding collection: {e}")

    def initialize_chromadb(self) -> None:
        """Initialize ChromaDB client and get collection."""
        try:
            # Create ChromaDB client with persistent storage
            db_path = KEYWORDS_EMBEDDING_DBPATH / "chromadb"
            if not db_path.exists():
                raise FileNotFoundError(f"ChromaDB directory not found: {db_path}")
            
            self.chroma_client = chromadb.PersistentClient(path=str(db_path))
            
            # Find the appropriate embedding collection
            self.collection_name = self.find_embedding_collection()
            
            # Get existing collection with embedding function
            # Get model name from vLLM server
            temp_client = OpenAI(api_key="EMPTY", base_url=self.vllm_base_url)
            models = temp_client.models.list()
            if not models.data:
                raise Exception("No models available on vLLM server")
            
            model_name = models.data[0].id
            embedding_function = OpenAIEmbeddingFunction(
                api_key="EMPTY",
                model_name=model_name,
                api_base=self.vllm_base_url
            )
            self.chroma_collection = self.chroma_client.get_collection(
                name=self.collection_name,
                embedding_function=embedding_function
            )
            
        except Exception as e:
            raise Exception(f"Failed to initialize ChromaDB for clustering: {e}")

    def load_embeddings_from_chromadb(self) -> Tuple[np.ndarray, List[str], Dict[str, Dict]]:
        """
        Load embeddings and keyword information directly from ChromaDB.
        
        Returns:
            Tuple of (embeddings array, keywords list, keyword metadata dict)
        """
        if self.chroma_collection is None:
            self.initialize_chromadb()
        
        # Get all data from collection
        results = self.chroma_collection.get(
            include=['embeddings', 'metadatas', 'documents']
        )
        
        if results['embeddings'] is None or len(results['embeddings']) == 0:
            raise ValueError("No embeddings found in ChromaDB collection")
        
        embeddings = np.array(results['embeddings'], dtype=np.float32)
        keywords = [metadata['keyword'] for metadata in results['metadatas']]
        
        # Reconstruct keyword_info dictionary from metadata
        keyword_info = {}
        for metadata in results['metadatas']:
            keyword = metadata['keyword']
            keyword_info[keyword] = {
                'frequency': metadata['frequency'],
                'num_objects': metadata['num_objects'],
                'keyword_objects': []  # We don't store full objects in ChromaDB metadata
            }
        
        return embeddings, keywords, keyword_info

    def run_dbscan_clustering(self, embeddings: np.ndarray, valid_keywords: List[str], keyword_info: Dict[str, Dict]) -> List[KeywordCluster]:
        """
        Run DBSCAN clustering on embeddings to generate cluster proposals.
        
        Args:
            embeddings: Normalized embeddings array
            valid_keywords: List of keywords corresponding to embeddings
            keyword_info: Dictionary of keyword information
            
        Returns:
            List of keyword clusters
        """
        print("üîÑ Running DBSCAN clustering...")
        
        # Apply DBSCAN clustering on embeddings
        clustering = DBSCAN(
            eps=1 - self.similarity_threshold,  # Convert similarity to distance
            min_samples=self.min_cluster_size,
            metric='cosine'
        ).fit(embeddings)
        
        # Group keywords by cluster
        clusters = defaultdict(list)
        for idx, label in enumerate(clustering.labels_):
            if label != -1:  # -1 indicates noise/outliers
                clusters[label].append(valid_keywords[idx])
        
        print(f"‚úÖ Found {len(clusters)} clusters from {len(valid_keywords)} keywords")
        
        # Create KeywordCluster objects with enhanced information
        cluster_objects = []
        for cluster_id, cluster_keywords in clusters.items():
            # Select canonical term (most frequent in cluster)
            canonical_term = max(cluster_keywords, 
                                key=lambda k: keyword_info[k]['frequency'])
            
            # Calculate cluster statistics
            total_frequency = sum(keyword_info[k]['frequency'] for k in cluster_keywords)
            
            # Collect all keyword objects for the cluster
            all_keyword_objects = []
            for kw in cluster_keywords:
                all_keyword_objects.extend(keyword_info[kw].get('keyword_objects', []))
            
            cluster_obj = KeywordCluster(
                canonical_term=canonical_term,
                variants=cluster_keywords,
                frequency=total_frequency
            )
            # Add enhanced information as attributes
            cluster_obj.keyword_objects = all_keyword_objects
            
            cluster_objects.append(cluster_obj)
        
        return cluster_objects

    def cluster_keywords(self) -> List[KeywordCluster]:
        """
        Cluster keywords using DBSCAN clustering with caching support.
        Loads embeddings directly from ChromaDB.
        
        Returns:
            List of keyword clusters
        """
        # Check if we can use cached clustering results
        clustering_path = KEYWORDS_EMBEDDING_DBPATH / "clustering.json"
        if clustering_path.exists():
            try:
                clusters = self.load_clustering_results()
                self.cached_clusters = clusters
                print(f"‚úÖ Using cached clustering results: {len(clusters)} clusters")
                return clusters
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load cached clustering results: {e}")
                print("Regenerating clustering...")

        # Load embeddings and keywords from ChromaDB
        embeddings, keywords, keyword_info = self.load_embeddings_from_chromadb()
        
        # Run DBSCAN clustering on embeddings
        clusters = self.run_dbscan_clustering(embeddings, keywords, keyword_info)
        
        # Cache clustering results
        print("üíæ Caching clustering results...")
        self.save_clustering_results(clusters)
        self.cached_clusters = clusters
        
        return clusters

    def save_clustering_results(self, clusters: List[KeywordCluster], clustering_path: Union[str, Path] = None) -> None:
        """
        Save clustering results to disk for caching.
        
        Args:
            clusters: List of KeywordCluster objects to save
            clustering_path: Path to save the clustering results. If None, uses EMBEDDINGS_DIR/clustering.json
        """
        if clustering_path is None:
            clustering_path = KEYWORDS_EMBEDDING_DBPATH / "clustering.json"
        
        # Ensure the embeddings directory exists
        KEYWORDS_EMBEDDING_DBPATH.mkdir(parents=True, exist_ok=True)
        
        # Convert KeywordCluster objects to serializable format
        serializable_clusters = []
        for cluster in clusters:
            cluster_data = {
                "canonical_term": cluster.canonical_term,
                "variants": cluster.variants,
                "frequency": cluster.frequency,
                "keyword_objects": getattr(cluster, 'keyword_objects', [])
            }
            serializable_clusters.append(cluster_data)
        
        with open(clustering_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_clusters, f, ensure_ascii=False)
        
        print(f"‚úÖ Clustering results saved to {clustering_path}")

    def load_clustering_results(self, clustering_path: Union[str, Path] = None) -> List[KeywordCluster]:
        """
        Load clustering results from disk.
        
        Args:
            clustering_path: Path to load the clustering results from. If None, uses EMBEDDINGS_DIR/clustering.json
            
        Returns:
            List of KeywordCluster objects
        """
        if clustering_path is None:
            clustering_path = KEYWORDS_EMBEDDING_DBPATH / "clustering.json"
        
        clustering_path = Path(clustering_path)
        
        if not clustering_path.exists():
            raise FileNotFoundError(f"Clustering results file not found: {clustering_path}")
        
        with open(clustering_path, 'r', encoding='utf-8') as f:
            serializable_clusters = json.load(f)
        
        # Convert back to KeywordCluster objects
        clusters = []
        for cluster_data in serializable_clusters:
            cluster = KeywordCluster(
                canonical_term=cluster_data["canonical_term"],
                variants=cluster_data["variants"],
                frequency=cluster_data["frequency"]
            )
            # Add enhanced information as attributes
            cluster.keyword_objects = cluster_data.get("keyword_objects", [])
            clusters.append(cluster)
        
        print(f"‚úÖ Clustering results loaded from {clustering_path}")
        
        return clusters

    def save_harmonized_results(self, results: List[KeywordCluster], output_file: Union[str, Path]) -> None:
        """
        Save harmonized results to JSON file with keyword objects maintained.
        
        Args:
            results: List of harmonized keyword clusters
            output_file: Output file path.
        """
        # Convert KeywordCluster objects to serializable format
        serializable_results = []
        
        for i, cluster in enumerate(results):
            cluster_data = {
                "cluster_id": i,
                "canonical_term": cluster.canonical_term,
                "variants": getattr(cluster, 'keyword_objects', []),
                "frequency": cluster.frequency,
            }
            serializable_results.append(cluster_data)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False)
        
        print(f"‚úÖ Harmonized results saved to {output_file}")

    def clear_clustering_cache(self) -> int:
        """
        Clear clustering results cache.
        
        Returns:
            Number of items removed
        """
        clustering_path = KEYWORDS_EMBEDDING_DBPATH / "clustering.json"
        
        removed_count = 0
        
        # Remove clustering cache
        if clustering_path.exists():
            try:
                clustering_path.unlink()
                removed_count += 1
                print(f"üóëÔ∏è Removed clustering cache: {clustering_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to remove {clustering_path}: {e}")
        
        # Clear instance variables
        self.cached_clusters = None
        
        return removed_count

    def run_full_pipeline(
        self,
        output_file: Union[str, Path] = CLUSTERS_PROPOSAL_PATH
    ) -> List[KeywordCluster]:
        """
        Run the complete clustering pipeline using existing ChromaDB embeddings.
        
        Args:
            output_file: Path to save clustering results
            
        Returns:
            List of keyword clusters
        """
        print("üöÄ Running clustering pipeline with ChromaDB embeddings...")
        
        # Cluster keywords (this will load from ChromaDB)
        clusters = self.cluster_keywords()
        
        # Save results
        self.save_harmonized_results(clusters, output_file)
        
        return clusters


def clear_all_cache() -> None:
    """
    Clear all caches (both embedding and clustering).
    """
    from embedding import EmbeddingManager
    
    embedding_manager = EmbeddingManager()
    clustering_manager = ClusteringManager()
    
    embedding_removed = embedding_manager.clear_embedding_cache()
    clustering_removed = clustering_manager.clear_clustering_cache()
    
    total_removed = embedding_removed + clustering_removed
    
    if total_removed == 0:
        print("‚ÑπÔ∏è No cache files found to remove")
    else:
        print(f"‚úÖ All caches cleared: {total_removed} items removed")


# Create Typer app
app = typer.Typer(help="Keyword clustering and harmonization tool using ChromaDB embeddings")


@app.command()
def cluster(
    output_file: str = typer.Option(
        str(CLUSTERS_PROPOSAL_PATH),
        "--output", "-o",
        help="Path to save clustering results"
    ),
    similarity_threshold: float = typer.Option(
        SIMILARITY_THRESHOLD,
        "--threshold", "-t",
        help="Similarity threshold for clustering (0.0-1.0)"
    ),
    min_cluster_size: int = typer.Option(
        MIN_CLUSTER_SIZE,
        "--min-size", "-s",
        help="Minimum cluster size for DBSCAN"
    ),
    vllm_url: str = typer.Option(
        "http://localhost:8000/v1",
        "--vllm-url",
        help="Base URL for vLLM embedding server"
    ),
    force_regenerate: bool = typer.Option(
        False,
        "--force",
        help="Force regenerate clusters even if cache exists"
    )
):
    """Run DBSCAN clustering on keyword embeddings from ChromaDB."""
    try:
        # Initialize clustering manager
        clustering_manager = ClusteringManager(
            similarity_threshold=similarity_threshold,
            min_cluster_size=min_cluster_size,
            vllm_base_url=vllm_url
        )
        
        if force_regenerate:
            # Clear clustering cache
            clustering_manager.clear_clustering_cache()
            typer.echo("üóëÔ∏è Cleared clustering cache")
        
        typer.echo("ÔøΩ Running clustering on ChromaDB embeddings...")
        clusters = clustering_manager.cluster_keywords()
        
        typer.echo(f"üíæ Saving clustering results to {output_file}...")
        clustering_manager.save_harmonized_results(clusters, output_file)
        
        typer.echo(f"‚úÖ Successfully generated {len(clusters)} clusters")
        
        # Print summary statistics
        total_keywords = sum(len(cluster.variants) for cluster in clusters)
        avg_cluster_size = total_keywords / len(clusters) if clusters else 0
        typer.echo(f"üìä Total keywords clustered: {total_keywords}")
        typer.echo(f"üìä Average cluster size: {avg_cluster_size:.2f}")
        
    except Exception as e:
        typer.echo(f"‚ùå Error during clustering: {e}", err=True)
        raise typer.Exit(1)


@app.command()
def map_keywords(
    clusters_file: str = typer.Option(
        str(CLUSTERS_PROPOSAL_PATH),
        "--clusters", "-c",
        help="Path to clustering results JSON file"
    ),
    keywords_file: str = typer.Option(
        str(EXTRACTED_KEYWORDS_PATH),
        "--keywords", "-k",
        help="Path to original keywords JSON file"
    ),
    output_file: str = typer.Option(
        "keyword_mapping.json",
        "--output", "-o",
        help="Path to save keyword mapping results"
    )
):
    """Map original extracted keywords to final harmonized keywords."""
    try:
        typer.echo(f"üìÑ Loading clustering results from {clusters_file}...")
        with open(clusters_file, 'r', encoding='utf-8') as f:
            clusters_data = json.load(f)
        
        typer.echo(f"üìÑ Loading original keywords from {keywords_file}...")
        with open(keywords_file, 'r', encoding='utf-8') as f:
            original_keywords = json.load(f)
        
        typer.echo("üó∫Ô∏è Creating keyword mapping...")
        
        # Create mapping from variant keywords to canonical terms
        keyword_mapping = {}
        cluster_info = {}
        
        for cluster in clusters_data:
            canonical = cluster.get('canonical_term', '')
            variants = cluster.get('variants', [])
            
            # Store cluster info
            cluster_info[canonical] = {
                'cluster_id': cluster.get('cluster_id', ''),
                'frequency': cluster.get('frequency', 0),
                'total_variants': len(variants)
            }
            
            # Map each variant to the canonical term
            for variant_obj in variants:
                if isinstance(variant_obj, dict):
                    variant_term = variant_obj.get('term', '')
                    if variant_term:
                        keyword_mapping[variant_term] = {
                            'canonical_term': canonical,
                            'cluster_id': cluster.get('cluster_id', ''),
                            'original_keyword': variant_obj
                        }
        
        # Create final mapping structure
        final_mapping = {
            'summary': {
                'total_clusters': len(clusters_data),
                'total_mapped_keywords': len(keyword_mapping),
                'total_original_keywords': len(original_keywords)
            },
            'cluster_info': cluster_info,
            'keyword_mapping': keyword_mapping,
            'unmapped_keywords': []
        }
        
        # Find unmapped keywords
        for kw in original_keywords:
            if isinstance(kw, dict):
                term = kw.get('term', '')
                if term and term not in keyword_mapping:
                    final_mapping['unmapped_keywords'].append(kw)
        
        typer.echo(f"üíæ Saving keyword mapping to {output_file}...")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_mapping, f, ensure_ascii=False, indent=2)
        
        # Print summary
        typer.echo("‚úÖ Keyword mapping completed!")
        typer.echo(f"üìä Mapped {len(keyword_mapping)} keywords to {len(clusters_data)} clusters")
        typer.echo(f"üìä {len(final_mapping['unmapped_keywords'])} keywords remained unmapped")
        
        if final_mapping['unmapped_keywords']:
            typer.echo("‚ö†Ô∏è Some keywords could not be mapped to clusters")
            typer.echo("   This typically happens with keywords that were filtered out during clustering")
        
    except Exception as e:
        typer.echo(f"‚ùå Error during keyword mapping: {e}", err=True)
        raise typer.Exit(1)


@app.command()
def pipeline(
    output_file: str = typer.Option(
        str(CLUSTERS_PROPOSAL_PATH),
        "--output", "-o",
        help="Path to save final results"
    ),
    similarity_threshold: float = typer.Option(
        SIMILARITY_THRESHOLD,
        "--threshold", "-t",
        help="Similarity threshold for clustering (0.0-1.0)"
    ),
    min_cluster_size: int = typer.Option(
        MIN_CLUSTER_SIZE,
        "--min-size", "-s",
        help="Minimum cluster size for DBSCAN"
    ),
    vllm_url: str = typer.Option(
        "http://localhost:8000/v1",
        "--vllm-url",
        help="Base URL for vLLM embedding server"
    ),
    clear_cache: bool = typer.Option(
        False,
        "--clear-cache",
        help="Clear all caches before running"
    )
):
    """Run the complete clustering pipeline using existing ChromaDB embeddings."""
    try:
        clustering_manager = ClusteringManager(
            similarity_threshold=similarity_threshold,
            min_cluster_size=min_cluster_size,
            vllm_base_url=vllm_url
        )
        
        if clear_cache:
            typer.echo("üóëÔ∏è Clearing all caches...")
            clear_all_cache()
        
        typer.echo("üöÄ Running clustering pipeline...")
        clusters = clustering_manager.run_full_pipeline(output_file=output_file)
        
        typer.echo(f"‚úÖ Pipeline completed successfully!")
        typer.echo(f"üìä Generated {len(clusters)} clusters")
        typer.echo(f"üíæ Results saved to {output_file}")
        
    except Exception as e:
        typer.echo(f"‚ùå Pipeline failed: {e}", err=True)
        raise typer.Exit(1)


@app.command()
def clear_cache():
    """Clear cached clustering results and ChromaDB embeddings data."""
    try:
        clear_all_cache()
        typer.echo("‚úÖ All caches cleared successfully!")
    except Exception as e:
        typer.echo(f"‚ùå Error clearing cache: {e}", err=True)
        raise typer.Exit(1)


if __name__ == "__main__":
    app()
